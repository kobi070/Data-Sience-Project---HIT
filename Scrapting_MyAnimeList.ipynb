{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kobi\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "### Importing relevent lib ###\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import scipy as sc\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all genre links to scrapt information of each genre Anime in myAnimeList ###\n",
    "def get_all_genre_links(site_url,genre_dic):\n",
    "    genre_url = []\n",
    "    for i in genre_dict:\n",
    "        genre_url.append(site_url + genre_dict[i]+ '/'+i)\n",
    "    return genre_url\n",
    "\n",
    "### All Genres in Site ###\n",
    "genre_dict = {\n",
    "    'Action': '1',\n",
    "    'Adventure':'2',\n",
    "    'Cars':'3',\n",
    "    'Comedy':'4',\n",
    "    'Dementia':'5',\n",
    "    'Demons':'6',\n",
    "    'Drama':'8',\n",
    "    'Ecchi':'9',\n",
    "    'Fantasy':'10',\n",
    "    'Game':'11',\n",
    "    'Historical':'13',\n",
    "    'Horror':'14',\n",
    "    'Kids':'15',\n",
    "    'Harem':'35',\n",
    "    'Josei':'43'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all top anime links\n",
    "def get_all_top_links(site_url):\n",
    "    all_top_urls = []\n",
    "    for i in range(0,19000,50):\n",
    "        all_top_urls.append(site_url+'?limit='+str(i))\n",
    "    return all_top_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a soup Object ###\n",
    "def load_soup_obj(html):\n",
    "    html_get = requests.get(html).text\n",
    "    soup = BeautifulSoup(html_get,'lxml')\n",
    "    return sou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Site's url's ###\n",
    "mal_genre_front_page = 'https://myanimelist.net/anime.php'\n",
    "top_anime_url = 'https://myanimelist.net/topanime.php'\n",
    "### All genre Links list ###\n",
    "mal_genre_links = get_all_genre_links(mal_genre_front_page,genre_dict)\n",
    "### All Top 0-19,000 Anime's in mal URL list ###\n",
    "top_anime_list = get_all_top_links(top_anime_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Engline The Episodes ###\n",
    "def parse_episodes(listt):\n",
    "    result = []\n",
    "    for i in listt[:2]:\n",
    "        r = i.strip()\n",
    "        result.append(r)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "1\n",
      "200\n",
      "2\n",
      "200\n",
      "3\n",
      "200\n",
      "4\n",
      "200\n",
      "5\n",
      "200\n",
      "6\n",
      "200\n",
      "7\n",
      "200\n",
      "8\n",
      "200\n",
      "9\n",
      "200\n",
      "10\n",
      "200\n",
      "11\n",
      "200\n",
      "12\n",
      "200\n",
      "13\n",
      "200\n",
      "14\n",
      "200\n",
      "15\n",
      "200\n",
      "16\n",
      "200\n",
      "17\n",
      "200\n",
      "18\n",
      "200\n",
      "19\n",
      "200\n",
      "20\n",
      "200\n",
      "21\n",
      "200\n",
      "22\n",
      "200\n",
      "23\n",
      "200\n",
      "24\n",
      "200\n",
      "25\n",
      "200\n",
      "26\n",
      "200\n",
      "27\n",
      "200\n",
      "28\n",
      "200\n",
      "29\n",
      "200\n",
      "30\n",
      "200\n",
      "31\n",
      "200\n",
      "32\n",
      "200\n",
      "33\n",
      "200\n",
      "34\n",
      "200\n",
      "35\n",
      "200\n",
      "36\n",
      "200\n",
      "37\n",
      "200\n",
      "38\n",
      "200\n",
      "39\n",
      "200\n",
      "40\n",
      "200\n",
      "41\n",
      "200\n",
      "42\n",
      "200\n",
      "43\n",
      "200\n",
      "44\n",
      "200\n",
      "45\n",
      "200\n",
      "46\n",
      "200\n",
      "47\n",
      "200\n",
      "48\n",
      "200\n",
      "49\n",
      "200\n",
      "50\n",
      "200\n",
      "51\n",
      "200\n",
      "52\n",
      "200\n",
      "53\n",
      "200\n",
      "54\n",
      "200\n",
      "55\n",
      "200\n",
      "56\n",
      "200\n",
      "57\n",
      "200\n",
      "58\n",
      "200\n",
      "59\n",
      "200\n",
      "60\n",
      "200\n",
      "61\n",
      "200\n",
      "62\n",
      "200\n",
      "63\n",
      "200\n",
      "64\n",
      "200\n",
      "65\n",
      "200\n",
      "66\n",
      "200\n",
      "67\n",
      "200\n",
      "68\n",
      "200\n",
      "69\n",
      "200\n",
      "70\n",
      "200\n",
      "71\n",
      "200\n",
      "72\n",
      "200\n",
      "73\n",
      "200\n",
      "74\n",
      "200\n",
      "75\n",
      "200\n",
      "76\n",
      "200\n",
      "77\n",
      "200\n",
      "78\n",
      "200\n",
      "79\n",
      "200\n",
      "80\n",
      "200\n",
      "81\n",
      "200\n",
      "82\n",
      "200\n",
      "83\n",
      "200\n",
      "84\n",
      "200\n",
      "85\n",
      "200\n",
      "86\n",
      "200\n",
      "87\n",
      "200\n",
      "88\n",
      "200\n",
      "89\n",
      "200\n",
      "90\n",
      "200\n",
      "91\n",
      "200\n",
      "92\n",
      "200\n",
      "93\n",
      "200\n",
      "94\n",
      "200\n",
      "95\n",
      "200\n",
      "96\n",
      "200\n",
      "97\n",
      "200\n",
      "98\n",
      "200\n",
      "99\n",
      "200\n",
      "100\n",
      "200\n",
      "101\n",
      "200\n",
      "102\n",
      "200\n",
      "103\n",
      "200\n",
      "104\n",
      "200\n",
      "105\n",
      "200\n",
      "106\n",
      "200\n",
      "107\n",
      "200\n",
      "108\n",
      "200\n",
      "109\n",
      "200\n",
      "110\n",
      "200\n",
      "111\n",
      "200\n",
      "112\n",
      "200\n",
      "113\n",
      "200\n",
      "114\n",
      "200\n",
      "115\n",
      "200\n",
      "116\n",
      "200\n",
      "117\n",
      "200\n",
      "118\n",
      "200\n",
      "119\n",
      "200\n",
      "120\n",
      "200\n",
      "121\n",
      "200\n",
      "122\n",
      "200\n",
      "123\n",
      "200\n",
      "124\n",
      "200\n",
      "125\n",
      "200\n",
      "126\n",
      "200\n",
      "127\n",
      "200\n",
      "128\n",
      "200\n",
      "129\n",
      "200\n",
      "130\n",
      "200\n",
      "131\n",
      "200\n",
      "132\n",
      "200\n",
      "133\n",
      "200\n",
      "134\n",
      "200\n",
      "135\n",
      "200\n",
      "136\n",
      "200\n",
      "137\n",
      "200\n",
      "138\n",
      "200\n",
      "139\n",
      "200\n",
      "140\n",
      "200\n",
      "141\n",
      "200\n",
      "142\n",
      "200\n",
      "143\n",
      "200\n",
      "144\n",
      "200\n",
      "145\n",
      "200\n",
      "146\n",
      "200\n",
      "147\n",
      "200\n",
      "148\n",
      "200\n",
      "149\n",
      "200\n",
      "150\n",
      "200\n",
      "151\n",
      "200\n",
      "152\n",
      "200\n",
      "153\n",
      "200\n",
      "154\n",
      "200\n",
      "155\n",
      "200\n",
      "156\n",
      "200\n",
      "157\n",
      "200\n",
      "158\n",
      "200\n",
      "159\n",
      "200\n",
      "160\n",
      "200\n",
      "161\n",
      "200\n",
      "162\n",
      "200\n",
      "163\n",
      "200\n",
      "164\n",
      "200\n",
      "165\n",
      "200\n",
      "166\n",
      "200\n",
      "167\n",
      "200\n",
      "168\n",
      "200\n",
      "169\n",
      "200\n",
      "170\n",
      "200\n",
      "171\n",
      "200\n",
      "172\n",
      "200\n",
      "173\n",
      "200\n",
      "174\n",
      "200\n",
      "175\n",
      "200\n",
      "176\n",
      "200\n",
      "177\n",
      "200\n",
      "178\n",
      "200\n",
      "179\n",
      "200\n",
      "180\n",
      "200\n",
      "181\n",
      "200\n",
      "182\n",
      "200\n",
      "183\n",
      "200\n",
      "184\n",
      "200\n",
      "185\n",
      "200\n",
      "186\n",
      "200\n",
      "187\n",
      "200\n",
      "188\n",
      "200\n",
      "189\n",
      "200\n",
      "190\n",
      "200\n",
      "191\n",
      "200\n",
      "192\n",
      "200\n",
      "193\n",
      "200\n",
      "194\n",
      "200\n",
      "195\n",
      "200\n",
      "196\n",
      "200\n",
      "197\n",
      "200\n",
      "198\n",
      "200\n",
      "199\n",
      "200\n",
      "200\n",
      "200\n",
      "201\n",
      "200\n",
      "202\n",
      "200\n",
      "203\n",
      "200\n",
      "204\n",
      "200\n",
      "205\n",
      "200\n",
      "206\n",
      "200\n",
      "207\n",
      "200\n",
      "208\n",
      "200\n",
      "209\n",
      "200\n",
      "210\n",
      "200\n",
      "211\n",
      "200\n",
      "212\n",
      "200\n",
      "213\n",
      "200\n",
      "214\n",
      "200\n",
      "215\n",
      "200\n",
      "216\n",
      "200\n",
      "217\n",
      "200\n",
      "218\n",
      "200\n",
      "219\n",
      "200\n",
      "220\n",
      "200\n",
      "221\n",
      "200\n",
      "222\n",
      "200\n",
      "223\n",
      "200\n",
      "224\n",
      "200\n",
      "225\n",
      "200\n",
      "226\n",
      "200\n",
      "227\n",
      "200\n",
      "228\n",
      "200\n",
      "229\n",
      "200\n",
      "230\n",
      "200\n",
      "231\n",
      "200\n",
      "232\n",
      "200\n",
      "233\n",
      "200\n",
      "234\n",
      "200\n",
      "235\n",
      "200\n",
      "236\n",
      "200\n",
      "237\n",
      "200\n",
      "238\n",
      "200\n",
      "239\n",
      "200\n",
      "240\n",
      "200\n",
      "241\n",
      "200\n",
      "242\n",
      "200\n",
      "243\n",
      "200\n",
      "244\n",
      "200\n",
      "245\n",
      "200\n",
      "246\n",
      "200\n",
      "247\n",
      "200\n",
      "248\n",
      "200\n",
      "249\n",
      "200\n",
      "250\n",
      "200\n",
      "251\n",
      "200\n",
      "252\n",
      "200\n",
      "253\n",
      "200\n",
      "254\n",
      "200\n",
      "255\n",
      "200\n",
      "256\n",
      "200\n",
      "257\n",
      "200\n",
      "258\n",
      "200\n",
      "259\n",
      "200\n",
      "260\n",
      "200\n",
      "261\n",
      "200\n",
      "262\n",
      "200\n",
      "263\n",
      "200\n",
      "264\n",
      "200\n",
      "265\n",
      "200\n",
      "266\n",
      "200\n",
      "267\n",
      "200\n",
      "268\n",
      "200\n",
      "269\n",
      "200\n",
      "270\n",
      "200\n",
      "271\n",
      "200\n",
      "272\n",
      "200\n",
      "273\n",
      "200\n",
      "274\n",
      "200\n",
      "275\n",
      "200\n",
      "276\n",
      "200\n",
      "277\n",
      "200\n",
      "278\n",
      "200\n",
      "279\n",
      "200\n",
      "280\n",
      "200\n",
      "281\n",
      "200\n",
      "282\n",
      "200\n",
      "283\n",
      "200\n",
      "284\n",
      "200\n",
      "285\n",
      "200\n",
      "286\n",
      "200\n",
      "287\n",
      "200\n",
      "288\n",
      "200\n",
      "289\n",
      "200\n",
      "290\n",
      "200\n",
      "291\n",
      "200\n",
      "292\n",
      "200\n",
      "293\n",
      "200\n",
      "294\n",
      "200\n",
      "295\n",
      "200\n",
      "296\n",
      "200\n",
      "297\n",
      "200\n",
      "298\n",
      "200\n",
      "299\n",
      "200\n",
      "300\n",
      "200\n",
      "301\n",
      "200\n",
      "302\n",
      "200\n",
      "303\n",
      "200\n",
      "304\n",
      "200\n",
      "305\n",
      "200\n",
      "306\n",
      "200\n",
      "307\n",
      "200\n",
      "308\n",
      "200\n",
      "309\n",
      "200\n",
      "310\n",
      "200\n",
      "311\n",
      "200\n",
      "312\n",
      "200\n",
      "313\n",
      "200\n",
      "314\n",
      "200\n",
      "315\n",
      "200\n",
      "316\n",
      "200\n",
      "317\n",
      "200\n",
      "318\n",
      "200\n",
      "319\n",
      "200\n",
      "320\n",
      "200\n",
      "321\n",
      "200\n",
      "322\n",
      "200\n",
      "323\n",
      "200\n",
      "324\n",
      "200\n",
      "325\n",
      "200\n",
      "326\n",
      "200\n",
      "327\n",
      "200\n",
      "328\n",
      "200\n",
      "329\n",
      "200\n",
      "330\n",
      "200\n",
      "331\n",
      "200\n",
      "332\n",
      "200\n",
      "333\n",
      "200\n",
      "334\n",
      "200\n",
      "335\n",
      "200\n",
      "336\n",
      "200\n",
      "337\n",
      "200\n",
      "338\n",
      "200\n",
      "339\n",
      "200\n",
      "340\n",
      "200\n",
      "341\n",
      "200\n",
      "342\n",
      "200\n",
      "343\n",
      "200\n",
      "344\n",
      "200\n",
      "345\n",
      "200\n",
      "346\n",
      "200\n",
      "347\n",
      "200\n",
      "348\n",
      "200\n",
      "349\n",
      "200\n",
      "350\n",
      "200\n",
      "351\n",
      "200\n",
      "352\n",
      "200\n",
      "353\n",
      "200\n",
      "354\n",
      "200\n",
      "355\n",
      "200\n",
      "356\n",
      "200\n",
      "357\n",
      "200\n",
      "358\n",
      "200\n",
      "359\n",
      "200\n",
      "360\n",
      "200\n",
      "361\n",
      "200\n",
      "362\n",
      "200\n",
      "363\n",
      "200\n",
      "364\n",
      "200\n",
      "365\n",
      "200\n",
      "366\n",
      "200\n",
      "367\n",
      "200\n",
      "368\n",
      "200\n",
      "369\n",
      "200\n",
      "370\n",
      "200\n",
      "371\n",
      "200\n",
      "372\n",
      "200\n",
      "373\n",
      "200\n",
      "374\n",
      "200\n",
      "375\n",
      "200\n",
      "376\n",
      "200\n",
      "377\n",
      "200\n",
      "378\n",
      "200\n",
      "379\n",
      "200\n",
      "380\n"
     ]
    }
   ],
   "source": [
    "top_anime = []\n",
    "top_anime_Rank = []\n",
    "top_anime_Title = []\n",
    "top_anime_Rating = []\n",
    "top_anime_URL = []\n",
    "top_anime_Image_URL = []\n",
    "top_anime_Episodes = []\n",
    "top_anime_Dates = []\n",
    "counter = 0\n",
    "\n",
    "### inspierd by https://blog.jovian.ai/scraping-data-using-beautiful-soup-and-python-4170e7ec63fd ###\n",
    "### inspierd by ###\n",
    "\n",
    "for link in top_anime_list:\n",
    "    \n",
    "    ### Load the soup object trough load_soup_obj() ###\n",
    "    html_get = requests.get(link)\n",
    "    t_html_get = html_get.text\n",
    "    counter+=1\n",
    "    print(html_get.status_code)\n",
    "    print(counter)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(t_html_get,'lxml')\n",
    "    soup.title.text.strip()\n",
    "    \n",
    "    ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "    headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "    ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "    row_content = soup.find_all('tr', {'class' : \"ranking-list\"})\n",
    "    #print(row_content)\n",
    "\n",
    "    for row in row_content:\n",
    "        episode = parse_episodes(row.find('div', class_ = \"information di-ib mt4\").text.strip().split('\\n'))\n",
    "    \n",
    "        top_anime_URL.append(row.find('td', class_= \"title al va-t word-break\").find('a')['href'])\n",
    "        top_anime_Rank.append(row.find('td', class_ = \"rank ac\").find('span').text)\n",
    "        top_anime_Title.append(row.find('div', class_=\"di-ib clearfix\").find('a').text)\n",
    "        top_anime_Rating.append(row.find('td', class_=\"score ac fs14\").find('span').text)\n",
    "        top_anime_Image_URL.append(row.find('td', class_ ='title al va-t word-break').find('img')['data-src'])\n",
    "        top_anime_Episodes = episode[0]\n",
    "        top_anime_Dates = episode[1]\n",
    "        \n",
    "        \n",
    "top_anime_df = pd.DataFrame({'Rank':top_anime_Rank,'Title':top_anime_Title,\n",
    "                                 'Rating':top_anime_Rating,'Image_URL':top_anime_Image_URL\n",
    "                                 ,'Episodes':top_anime_Episodes,'Dates':top_anime_Dates,\n",
    "                                'URL':top_anime_URL},)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        OVA (3 eps)\n",
       "1        OVA (3 eps)\n",
       "2        OVA (3 eps)\n",
       "3        OVA (3 eps)\n",
       "4        OVA (3 eps)\n",
       "            ...     \n",
       "18995    OVA (3 eps)\n",
       "18996    OVA (3 eps)\n",
       "18997    OVA (3 eps)\n",
       "18998    OVA (3 eps)\n",
       "18999    OVA (3 eps)\n",
       "Name: Episodes, Length: 19000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(top_anime_df['Episodes'])\n",
    "top_anime_df.to_csv('top_anime_df_v1.csv')\n",
    "\n",
    "\n",
    "# display(top_anime_df.describe())\n",
    "\n",
    "top_anime_df_copy = top_anime_df.copy()\n",
    "\n",
    "\n",
    "# list_top_anime_urls = top_anime_df_copy['URL'].to_list()\n",
    "# counter = 0\n",
    "# for l in list_top_anime_urls:\n",
    "\n",
    "#     ### Load the soup object trough load_soup_obj() ###\n",
    "#     html_get = requests.get(l)\n",
    "#     t_html_get = html_get.text\n",
    "    \n",
    "#     counter+=1\n",
    "#     print(html_get.status_code)\n",
    "#     print(counter)\n",
    "#     time.sleep(2)\n",
    "    \n",
    "#     soup = BeautifulSoup(t_html_get,'lxml')\n",
    "#     soup.title.text.strip()\n",
    "\n",
    "#     ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "#     headers_mal2 = soup.find_all('td', class_ = 'borderClass')\n",
    "#     ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "#     row_content2 = soup.find_all('div', {\"class\":'spaceit_pad'})\n",
    "\n",
    "\n",
    "#     information = []\n",
    "\n",
    "#     for row in row_content2:\n",
    "#         if row.find('a') != None:\n",
    "#             information.append(row.find('a').text)\n",
    "\n",
    "# print(information[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_top_anime_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bd82f29400f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m### Load the soup object trough load_soup_obj() ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhtml_get\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_top_anime_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mt_html_get\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml_get\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_top_anime_urls' is not defined"
     ]
    }
   ],
   "source": [
    "top_anime_df\n",
    "\n",
    "\n",
    "### Load the soup object trough load_soup_obj() ###\n",
    "html_get = requests.get(list_top_anime_urls[0])\n",
    "t_html_get = html_get.text\n",
    "    \n",
    "\n",
    "time.sleep(2)\n",
    "    \n",
    "soup = BeautifulSoup(t_html_get,'lxml')\n",
    "soup.title.text.strip()\n",
    "\n",
    "### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "headers_mal2 = soup.find_all('td', class_ = 'borderClass')\n",
    "### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "row_content2 = soup.find_all('div', {\"class\":'spaceit_pad'})\n",
    "\n",
    "\n",
    "information = []\n",
    "\n",
    "for row in row_content2:\n",
    "    if row.find('a') != None:\n",
    "        information.append(row.find('a').text)\n",
    "print(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Take Top Anime col and transfet to DataFrame ###\n",
    "# def top_anime_to_df(row_con):\n",
    "#     top_anime = []\n",
    "#     top_anime_Rank = []\n",
    "#     top_anime_Title = []\n",
    "#     top_anime_Rating = []\n",
    "#     top_anime_URL = []\n",
    "#     top_anime_Image_URL = []\n",
    "#     top_anime_Episodes = []\n",
    "#     top_anime_Dates = []\n",
    "\n",
    "#     for row in row_content:\n",
    "#         episode = parse_episodes(row.find('div', class_ = \"information di-ib mt4\").text.strip().split('\\n'))\n",
    "#         top_anime_URL.append(row.find('td', class_= \"title al va-t word-break\").find('a')['href'])\n",
    "#         top_anime_Rank.append(row.find('td', class_ = \"rank ac\").find('span').text)\n",
    "#         top_anime_Title.append(row.find('div', class_=\"di-ib clearfix\").find('a').text)\n",
    "#         top_anime_Rating.append(row.find('td', class_=\"score ac fs14\").find('span').text)\n",
    "#         top_anime_Image_URL.append(row.find('td', class_ ='title al va-t word-break').find('img')['data-src'])\n",
    "#         top_anime_Episodes = episode[0]\n",
    "#         top_anime_Dates = episode[1]\n",
    "    \n",
    "#         top_anime_df = pd.DataFrame({'Rank':top_anime_Rank,'Title':top_anime_Title,\n",
    "#                                      'Rating':top_anime_Rating,'Image_URL':top_anime_Image_URL\n",
    "#                                      ,'Episodes':top_anime_Episodes,'Dates':top_anime_Dates,\n",
    "#                                         'URL':top_anime_URL})\n",
    "    \n",
    "#     return top_anime_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### load top anime sites to transfer to DataFrame ###\n",
    "# def load_top_anime_html(site):    \n",
    "#     ### Load the soup object trough load_soup_obj() ###\n",
    "#     soup = load_soup_obj(site)\n",
    "#     soup.title.text.strip()\n",
    "#     ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "#     headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "#     ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "#     row_content = soup.find_all('tr', {'class' : \"ranking-list\"})\n",
    "#     return row_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bs_to_dic(row_content):\n",
    "#     top_anime = []\n",
    "#     top_anime_Rank = []\n",
    "#     top_anime_Title = []\n",
    "#     top_anime_Rating = []\n",
    "#     top_anime_URL = []\n",
    "#     top_anime_Image_URL = []\n",
    "#     top_anime_Episodes = []\n",
    "#     top_anime_Dates = []\n",
    "\n",
    "#     for row in row_content:\n",
    "#         episode = parse_episodes(row.find('div', class_ = \"information di-ib mt4\").text.strip().split('\\n'))\n",
    "    \n",
    "#         top_anime_URL.append(row.find('td', class_= \"title al va-t word-break\").find('a')['href'])\n",
    "#         top_anime_Rank.append(row.find('td', class_ = \"rank ac\").find('span').text)\n",
    "#         top_anime_Title.append(row.find('div', class_=\"di-ib clearfix\").find('a').text)\n",
    "#         top_anime_Rating.append(row.find('td', class_=\"score ac fs14\").find('span').text)\n",
    "#         top_anime_Image_URL.append(row.find('td', class_ ='title al va-t word-break').find('img')['data-src'])\n",
    "#         top_anime_Episodes = episode[0]\n",
    "#         top_anime_Dates = episode[1]\n",
    "    \n",
    "#     top_anime_dic = {'Rank':top_anime_Rank,'Title':top_anime_Title,\n",
    "#                                  'Rating':top_anime_Rating,'Image_URL':top_anime_Image_URL\n",
    "#                                  ,'Episodes':top_anime_Episodes,'Dates':top_anime_Dates,\n",
    "#                                 'URL':top_anime_URL}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Selenium Go Trough Pages ###\n",
    "\n",
    "# ### Open Certain Page in Selenium ###\n",
    "# PATH = \"C:/Users/Kobi/Downloads/chromedriver.exe\"\n",
    "# driver = webdriver.Chrome(PATH)\n",
    "# driver.get('https://myanimelist.net/topanime.php')\n",
    "\n",
    "# ### Lists ###\n",
    "# top_anime = []\n",
    "# top_anime_Rank = []\n",
    "# top_anime_Title = []\n",
    "# top_anime_Rating = []\n",
    "# top_anime_URL = []\n",
    "# top_anime_Image_URL = []\n",
    "# top_anime_Episodes = []\n",
    "# top_anime_Dates = []\n",
    "\n",
    "\n",
    "# html = driver.page_source\n",
    "# soup = BeautifulSoup(html)\n",
    "\n",
    "# soup.title.text.strip()\n",
    "\n",
    "\n",
    "# ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "# headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "# ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "# row_content = soup.find_all('tr', {'class' : \"ranking-list\"})\n",
    "\n",
    "# ### Fill Lists ###\n",
    "# top_anime_dic = bs_to_dic(row_content=row_content)\n",
    "\n",
    "# time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "#     main = WebDriverWait(driver, 20).until(EC.presence_of_element_located(\n",
    "#         (By.CLASS_NAME,'pb12')))\n",
    "#     link = driver.find_element_by_link_text(\"Next 50\")\n",
    "#     link.click()\n",
    "    \n",
    "# finally:\n",
    "#     driver.quit()\n",
    "\n",
    "\n",
    "# # link = driver.find_element_by_link_text('Next 50')\n",
    "# # link.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "# for i,_ in enumerate(top_anime_list):\n",
    "#     html = load_top_anime_html(_)\n",
    "#     res = html_to_lists(html)\n",
    "#     if (i>=1):\n",
    "#         df_new = html_to_lists(html[i])\n",
    "#         df_merged = pd.merge(res,df_new)\n",
    "    \n",
    "\n",
    "#res = load_top_anime(top_anime_list[1])\n",
    "#print(top_anime_list[1])\n",
    "#merged_df = pd.merge(top_anime_df_2,res,on=['Rank', 'Title', 'Rating', 'Image_URL', 'Episodes', 'Dates', 'URL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load the soup object trough load_soup_obj() ###\n",
    "# soup = load_soup_obj('https://myanimelist.net/topanime.php?limit=100')\n",
    "# soup.title.text.strip()\n",
    "# ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "# headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "# ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "# row_content = soup.find_all('tr', {'class' : \"ranking-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Rank       200 non-null    object\n",
      " 1   Title      200 non-null    object\n",
      " 2   Rating     200 non-null    object\n",
      " 3   Image_URL  200 non-null    object\n",
      " 4   Episodes   200 non-null    object\n",
      " 5   Dates      200 non-null    object\n",
      " 6   URL        200 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
