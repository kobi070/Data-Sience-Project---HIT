{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing relevent lib ###\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all genre links to scrapt information of each genre Anime in myAnimeList ###\n",
    "def get_all_genre_links(site_url,genre_dic):\n",
    "    genre_url = []\n",
    "    for i in genre_dict:\n",
    "        genre_url.append(site_url + genre_dict[i]+ '/'+i)\n",
    "    return genre_url\n",
    "\n",
    "### All Genres in Site ###\n",
    "genre_dict = {\n",
    "    'Action': '1',\n",
    "    'Adventure':'2',\n",
    "    'Cars':'3',\n",
    "    'Comedy':'4',\n",
    "    'Dementia':'5',\n",
    "    'Demons':'6',\n",
    "    'Drama':'8',\n",
    "    'Ecchi':'9',\n",
    "    'Fantasy':'10',\n",
    "    'Game':'11',\n",
    "    'Historical':'13',\n",
    "    'Horror':'14',\n",
    "    'Kids':'15',\n",
    "    'Harem':'35',\n",
    "    'Josei':'43'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get all top anime links ###\n",
    "### Sample works on 10,000 diffrent anime ###\n",
    "def get_all_top_links(site_url):\n",
    "    all_top_urls = []\n",
    "    for i in range(0,10000,50):\n",
    "        all_top_urls.append(site_url+'?limit='+str(i))\n",
    "    return all_top_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a soup Object ###\n",
    "def load_soup_obj(html):\n",
    "    html_get = requests.get(html).text\n",
    "    soup = BeautifulSoup(html_get,'lxml')\n",
    "    return sou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Site's url's ###\n",
    "mal_genre_front_page = 'https://myanimelist.net/anime.php'\n",
    "top_anime_url = 'https://myanimelist.net/topanime.php'\n",
    "### All genre Links list ###\n",
    "mal_genre_links = get_all_genre_links(mal_genre_front_page,genre_dict)\n",
    "### All Top 0-19,000 Anime's in mal URL list ###\n",
    "top_anime_list = get_all_top_links(top_anime_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Engline The Episodes ###\n",
    "def parse_episodes(listt):\n",
    "    result = []\n",
    "    for i in listt[:3]:\n",
    "        r = i.strip()\n",
    "#         print(r)\n",
    "        result.append(r)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_ep_string(list_ep):\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for i,_ in enumerate(list_ep):\n",
    "        search_TV = re.search('TV',list_ep[i])\n",
    "        search_OVA = re.search('OVA',list_ep[i])\n",
    "        search_Movie = re.search('Movie',list_ep[i])\n",
    "        if search_TV:\n",
    "            a = list_ep[i].split('TV')[1]\n",
    "            res.append(a)\n",
    "        elif search_OVA:\n",
    "            a = list_ep[i].split('OVA')[1]\n",
    "            res.append(a)\n",
    "        elif search_Movie:\n",
    "            a = list_ep[i].split('Movie')[1]\n",
    "            res.append(a)\n",
    "        else:\n",
    "            res.append(np.nan)\n",
    "            continue\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_members_string(list_mem):\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i,_ in enumerate(list_mem):\n",
    "        search = re.search('members',list_mem[i])\n",
    "        if search:\n",
    "            a = list_mem[i].split('members')[0]\n",
    "            res.append(a)    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dates(list_dates):\n",
    "    result_s = []\n",
    "    result_f = []\n",
    "    \n",
    "    \n",
    "    for i,_ in enumerate(list_dates):\n",
    "        res_s = list_dates[i].split('-')[0]\n",
    "        res_f = list_dates[i].split('-')[1]\n",
    "        result_s.append(res_s)\n",
    "        result_f.append(res_f)\n",
    "    \n",
    "    result = {'Start':result_s,'Finish':result_f}\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_type(list_ep):\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for i,_ in enumerate(list_ep):\n",
    "        search_TV = re.search('TV',list_ep[i])\n",
    "        search_OVA = re.search('OVA',list_ep[i])\n",
    "        search_Movie = re.search('Movie',list_ep[i])\n",
    "        if search_TV:\n",
    "#             a = list_ep[i].split('TV')[1]\n",
    "            res.append(search_TV.group(0))\n",
    "        elif search_OVA:\n",
    "#             a = list_ep[i].split('OVA')[1]\n",
    "            res.append(search_OVA.group(0))\n",
    "        elif search_Movie:\n",
    "#             a = list_ep[i].split('Movie')[1]\n",
    "            res.append(search_Movie.group(0))\n",
    "        else:\n",
    "            res.append(np.nan)\n",
    "            continue\n",
    "    return res\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_anime = []\n",
    "top_anime_Rank = []\n",
    "top_anime_Title = []\n",
    "top_anime_Rating = []\n",
    "top_anime_URL = []\n",
    "top_anime_Image_URL = []\n",
    "top_anime_Episodes = []\n",
    "top_anime_Dates = []\n",
    "top_anime_m_count = []\n",
    "try_list = []\n",
    "counter = 0\n",
    "\n",
    "### inspierd by https://blog.jovian.ai/scraping-data-using-beautiful-soup-and-python-4170e7ec63fd ###\n",
    "### inspierd by ###\n",
    "\n",
    "for link in top_anime_list:\n",
    "    \n",
    "    ### Load the soup object trough load_soup_obj() ###\n",
    "    html_get = requests.get(link)\n",
    "    t_html_get = html_get.text\n",
    "    counter+=1\n",
    "    if html_get.status_code != 200:\n",
    "            print('Status code:', html_get.status_code)\n",
    "    \n",
    "    print(counter)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(t_html_get,'lxml')\n",
    "    soup.title.text.strip()\n",
    "    \n",
    "    ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "    headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "    ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "    row_content = soup.find_all('tr', {'class' : \"ranking-list\"})\n",
    "    #print(row_content)\n",
    "\n",
    "    for row in row_content:\n",
    "        episode = parse_episodes(row.find('div', class_ = \"information di-ib mt4\").text.strip().split('\\n'))\n",
    "\n",
    "        \n",
    "        \n",
    "        top_anime_URL.append(row.find('td', class_= \"title al va-t word-break\").find('a')['href'])\n",
    "        top_anime_Rank.append(row.find('td', class_ = \"rank ac\").find('span').text)\n",
    "        top_anime_Title.append(row.find('div', class_=\"di-ib clearfix\").find('a').text)\n",
    "        top_anime_Rating.append(row.find('td', class_=\"score ac fs14\").find('span').text)\n",
    "        top_anime_Image_URL.append(row.find('td', class_ ='title al va-t word-break').find('img')['data-src'])\n",
    "        top_anime_Episodes.append(episode[0])\n",
    "        top_anime_Dates.append(episode[1]) \n",
    "        top_anime_m_count.append(episode[2])\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "top_anime_df = pd.DataFrame({'Rank':top_anime_Rank,'Title':top_anime_Title,\n",
    "                                 'Rating':top_anime_Rating,'Image_URL':top_anime_Image_URL\n",
    "                                 ,'Episodes':top_anime_Episodes,'Dates':top_anime_Dates,\n",
    "                                'URL':top_anime_URL,'Members_Votes':top_anime_m_count},)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### First Handle of Data ###\n",
    "copy = top_anime_df.copy()\n",
    "list_dates = copy.Dates.to_list()\n",
    "top_anime_df['Episodes'][0]\n",
    "list_types = split_type(top_anime_df['Episodes'])\n",
    "\n",
    "\n",
    "### Handle Data ###\n",
    "copy['Type'] = list_types\n",
    "copy['Members_Votes'] = delete_members_string(copy['Members_Votes'])\n",
    "copy['Episodes'] = delete_ep_string(copy['Episodes'])\n",
    "copy.set_index('Rank',inplace=True)\n",
    "dic_dates = split_dates(list_dates)\n",
    "copy['Start Date'] = dic_dates['Start']\n",
    "copy['Finish Date'] = dic_dates['Finish']\n",
    "copy.drop('Image_URL',axis=1, inplace=True)\n",
    "copy.drop('Dates',axis=1, inplace=True)\n",
    "\n",
    "\n",
    "copy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Episodes</th>\n",
       "      <th>URL</th>\n",
       "      <th>Members_Votes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Finish Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>9.15</td>\n",
       "      <td>(64 eps)</td>\n",
       "      <td>https://myanimelist.net/anime/5114/Fullmetal_A...</td>\n",
       "      <td>2,720,933</td>\n",
       "      <td>TV</td>\n",
       "      <td>Apr 2009</td>\n",
       "      <td>Jul 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gintama°</td>\n",
       "      <td>9.09</td>\n",
       "      <td>(51 eps)</td>\n",
       "      <td>https://myanimelist.net/anime/28977/Gintama°</td>\n",
       "      <td>495,687</td>\n",
       "      <td>TV</td>\n",
       "      <td>Apr 2015</td>\n",
       "      <td>Mar 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shingeki no Kyojin Season 3 Part 2</td>\n",
       "      <td>9.09</td>\n",
       "      <td>(10 eps)</td>\n",
       "      <td>https://myanimelist.net/anime/38524/Shingeki_n...</td>\n",
       "      <td>1,650,249</td>\n",
       "      <td>TV</td>\n",
       "      <td>Apr 2019</td>\n",
       "      <td>Jul 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steins;Gate</td>\n",
       "      <td>9.09</td>\n",
       "      <td>(24 eps)</td>\n",
       "      <td>https://myanimelist.net/anime/9253/Steins_Gate</td>\n",
       "      <td>2,122,353</td>\n",
       "      <td>TV</td>\n",
       "      <td>Apr 2011</td>\n",
       "      <td>Sep 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fruits Basket: The Final</td>\n",
       "      <td>9.06</td>\n",
       "      <td>(13 eps)</td>\n",
       "      <td>https://myanimelist.net/anime/42938/Fruits_Bas...</td>\n",
       "      <td>293,483</td>\n",
       "      <td>TV</td>\n",
       "      <td>Apr 2021</td>\n",
       "      <td>Jun 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Idol Project</td>\n",
       "      <td>5.63</td>\n",
       "      <td>(4 eps)</td>\n",
       "      <td>https://myanimelist.net/anime/2072/Idol_Project</td>\n",
       "      <td>1,883</td>\n",
       "      <td>OVA</td>\n",
       "      <td>Sep 1995</td>\n",
       "      <td>Oct 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Kekkaishi</td>\n",
       "      <td>5.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://myanimelist.net/anime/40347/Kekkaishi</td>\n",
       "      <td>471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Kodomo no Keijijougaku</td>\n",
       "      <td>5.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://myanimelist.net/anime/8333/Kodomo_no_K...</td>\n",
       "      <td>1,022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oct 2007</td>\n",
       "      <td>Oct 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Kyoshin to Hyouka no Shiro</td>\n",
       "      <td>5.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://myanimelist.net/anime/42057/Kyoshin_to...</td>\n",
       "      <td>2,590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aug 2020</td>\n",
       "      <td>Dec 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Larva Island</td>\n",
       "      <td>5.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://myanimelist.net/anime/38446/Larva_Island</td>\n",
       "      <td>425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oct 2018</td>\n",
       "      <td>Oct 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  Rating   Episodes  \\\n",
       "Rank                                                           \n",
       "1        Fullmetal Alchemist: Brotherhood    9.15   (64 eps)   \n",
       "2                                Gintama°    9.09   (51 eps)   \n",
       "3      Shingeki no Kyojin Season 3 Part 2    9.09   (10 eps)   \n",
       "4                             Steins;Gate    9.09   (24 eps)   \n",
       "5                Fruits Basket: The Final    9.06   (13 eps)   \n",
       "...                                   ...     ...        ...   \n",
       "9996                         Idol Project    5.63    (4 eps)   \n",
       "9997                            Kekkaishi    5.63        NaN   \n",
       "9998               Kodomo no Keijijougaku    5.63        NaN   \n",
       "9999           Kyoshin to Hyouka no Shiro    5.63        NaN   \n",
       "10000                        Larva Island    5.63        NaN   \n",
       "\n",
       "                                                     URL Members_Votes Type  \\\n",
       "Rank                                                                          \n",
       "1      https://myanimelist.net/anime/5114/Fullmetal_A...    2,720,933    TV   \n",
       "2           https://myanimelist.net/anime/28977/Gintama°      495,687    TV   \n",
       "3      https://myanimelist.net/anime/38524/Shingeki_n...    1,650,249    TV   \n",
       "4         https://myanimelist.net/anime/9253/Steins_Gate    2,122,353    TV   \n",
       "5      https://myanimelist.net/anime/42938/Fruits_Bas...      293,483    TV   \n",
       "...                                                  ...           ...  ...   \n",
       "9996     https://myanimelist.net/anime/2072/Idol_Project        1,883   OVA   \n",
       "9997       https://myanimelist.net/anime/40347/Kekkaishi          471   NaN   \n",
       "9998   https://myanimelist.net/anime/8333/Kodomo_no_K...        1,022   NaN   \n",
       "9999   https://myanimelist.net/anime/42057/Kyoshin_to...        2,590   NaN   \n",
       "10000   https://myanimelist.net/anime/38446/Larva_Island          425   NaN   \n",
       "\n",
       "      Start Date Finish Date  \n",
       "Rank                          \n",
       "1      Apr 2009     Jul 2010  \n",
       "2      Apr 2015     Mar 2016  \n",
       "3      Apr 2019     Jul 2019  \n",
       "4      Apr 2011     Sep 2011  \n",
       "5      Apr 2021     Jun 2021  \n",
       "...          ...         ...  \n",
       "9996   Sep 1995     Oct 1997  \n",
       "9997       2004         2004  \n",
       "9998   Oct 2007     Oct 2007  \n",
       "9999   Aug 2020     Dec 2020  \n",
       "10000  Oct 2018     Oct 2018  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "read_copy = pd.read_csv('Anime_DataFrame_VA_1.csv',index_col='Rank')\n",
    "read_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Rank           100 non-null    int64  \n",
      " 1   Title          100 non-null    object \n",
      " 2   Rating         100 non-null    float64\n",
      " 3   Image_URL      100 non-null    object \n",
      " 4   Episodes       100 non-null    object \n",
      " 5   Dates          100 non-null    object \n",
      " 6   URL            100 non-null    object \n",
      " 7   Members_Votes  100 non-null    object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres: Action, Adventure, Comedy, Drama, Fantasy\n",
      " Action, Adventure, Comedy, Drama, Fantasy\n"
     ]
    }
   ],
   "source": [
    "# s = str('Genres: Action, Adventure, Comedy, Drama, Fantasy')\n",
    "# print(s)\n",
    "\n",
    "# s2 = s.replace('Genres:','')\n",
    "# print(s2)\n",
    "# # str.replace('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        OVA (3 eps)\n",
       "1        OVA (3 eps)\n",
       "2        OVA (3 eps)\n",
       "3        OVA (3 eps)\n",
       "4        OVA (3 eps)\n",
       "            ...     \n",
       "18995    OVA (3 eps)\n",
       "18996    OVA (3 eps)\n",
       "18997    OVA (3 eps)\n",
       "18998    OVA (3 eps)\n",
       "18999    OVA (3 eps)\n",
       "Name: Episodes, Length: 19000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(top_anime_df)\n",
    "# # top_anime_df.to_csv('top_anime_df_v1.csv',)\n",
    "\n",
    "\n",
    "# # display(top_anime_df.describe())\n",
    "\n",
    "# top_anime_df_copy = top_anime_df.copy()\n",
    "\n",
    "\n",
    "# list_top_anime_urls = top_anime_df_copy['URL'].to_list()\n",
    "# counter = 0\n",
    "\n",
    "# for l in list_top_anime_urls:\n",
    "\n",
    "#     ### Load the soup object trough load_soup_obj() ###\n",
    "#     html_get = requests.get(l)\n",
    "#     t_html_get = html_get.text\n",
    "    \n",
    "    \n",
    "#     counter+=1\n",
    "#     print(html_get.status_code)\n",
    "#     print(counter)\n",
    "#     time.sleep(2)\n",
    "    \n",
    "#     soup = BeautifulSoup(t_html_get,'lxml')\n",
    "#     soup.title.text.strip()\n",
    "\n",
    "#     ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "#     headers_mal2 = soup.find_all('td', class_ = 'borderClass')\n",
    "#     ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "#     row_content2 = soup.find_all('div', {\"class\":'spaceit_pad'})\n",
    "\n",
    "\n",
    "#     information = []\n",
    "\n",
    "#     for row in row_content2:\n",
    "#         if row.find('a') != None:\n",
    "#             information.append(row.find('a').text)\n",
    "\n",
    "# print(information[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fullmetal Alchemist: Brotherhood.5114\n",
      "Gintama°.28977\n",
      "Shingeki no Kyojin Season 3 Part 2.38524\n",
      "Steins;Gate.9253\n",
      "Fruits Basket: The Final.42938\n",
      "Gintama'.7472\n",
      "Hunter x Hunter (2011).11061\n",
      "Ginga Eiyuu Densetsu.820\n",
      "Gintama': Enchousen.15417\n",
      "Gintama: The Final.15335\n",
      "Gintama..7472\n",
      "Violet Evergarden Movie.37987\n",
      "3-gatsu no Lion 2nd Season.35180\n",
      "Koe no Katachi.28851\n",
      "Clannad: After Story.4181\n",
      "Gintama.7472\n",
      "Gintama Movie 2: Kanketsu-hen - Yorozuya yo Eien Nare.15335\n",
      "Code Geass: Hangyaku no Lelouch R2.5163\n",
      "Owarimonogatari 2nd Season.35247\n",
      "Shingeki no Kyojin: The Final Season.40028\n",
      "Kimi no Na wa..32281\n",
      "Gintama.: Shirogane no Tamashii-hen - Kouhan-sen.37491\n",
      "Ousama Ranking.40834\n",
      "Shiguang Dailiren.44074\n",
      "Gintama.: Shirogane no Tamashii-hen.37491\n",
      "Haikyuu!!: Karasuno Koukou vs. Shiratorizawa Gakuen Koukou.32935\n",
      "Kizumonogatari III: Reiketsu-hen.31758\n",
      "Mob Psycho 100 II.50172\n",
      "Monster.14227\n",
      "Mushoku Tensei: Isekai Ittara Honki Dasu Part 2.45576\n",
      "Sen to Chihiro no Kamikakushi.199\n",
      "Monogatari Series: Second Season.17074\n",
      "Odd Taxi.46102\n",
      "Cowboy Bebop.5\n",
      "Kingdom 3rd Season.40682\n",
      "Shouwa Genroku Rakugo Shinjuu: Sukeroku Futatabi-hen.33095\n",
      "Mushishi Zoku Shou 2nd Season.24701\n",
      "Fate/stay night Movie: Heaven's Feel - III. Spring Song.33050\n",
      "Hajime no Ippo.19647\n",
      "Mo Dao Zu Shi: Wanjie Pian.40434\n",
      "Vinland Saga.37521\n",
      "Rurouni Kenshin: Meiji Kenkaku Romantan - Tsuioku-hen.44\n",
      "Jujutsu Kaisen (TV).40748\n",
      "Code Geass: Hangyaku no Lelouch.1575\n",
      "Mushishi Zoku Shou.21939\n",
      "Kimetsu no Yaiba Movie: Mugen Ressha-hen.40456\n",
      "Ashita no Joe 2.2921\n",
      "Great Teacher Onizuka.245\n",
      "Made in Abyss.37514\n",
      "Made in Abyss Movie 3: Fukaki Tamashii no Reimei.36862\n",
      "Mononoke Hime.164\n",
      "Mushishi.457\n",
      "Shigatsu wa Kimi no Uso.23273\n",
      "Evangelion: 3.0+1.0 Thrice Upon a Time.3786\n",
      "JoJo no Kimyou na Bouken Part 6: Stone Ocean.48661\n",
      "Haikyuu!! Second Season.28891\n",
      "Hajime no Ippo: New Challenger.5258\n",
      "Howl no Ugoku Shiro.431\n",
      "Kaguya-sama wa Kokurasetai?: Tensai-tachi no Renai Zunousen.37999\n",
      "Natsume Yuujinchou Shi.11665\n",
      "Violet Evergarden.37987\n",
      "Seishun Buta Yarou wa Yumemiru Shoujo no Yume wo Minai.38329\n",
      "Tengen Toppa Gurren Lagann.2001\n",
      "Natsume Yuujinchou Roku.34591\n",
      "Suzumiya Haruhi no Shoushitsu.7311\n",
      "Death Note.2994\n",
      "Kimetsu no Yaiba: Yuukaku-hen.47778\n",
      "One Piece.459\n",
      "Ping Pong the Animation.22135\n",
      "Shingeki no Kyojin Season 3.35760\n",
      "Mushishi Zoku Shou: Suzu no Shizuku.28957\n",
      "Ookami Kodomo no Ame to Yuki.12355\n",
      "Kizumonogatari II: Nekketsu-hen.31757\n",
      "JoJo no Kimyou na Bouken Part 5: Ougon no Kaze.37991\n",
      "Shouwa Genroku Rakugo Shinjuu.28735\n",
      "Natsume Yuujinchou Go.32983\n",
      "Natsume Yuujinchou San.10379\n",
      "Yojouhan Shinwa Taikei.7785\n",
      "Fate/Zero 2nd Season.11741\n",
      "Hajime no Ippo: Rising.19647\n",
      "Kimi no Suizou wo Tabetai.36098\n",
      "Fruits Basket 2nd Season.40417\n",
      "Kimetsu no Yaiba.38000\n",
      "Tengen Toppa Gurren Lagann Movie 2: Lagann-hen.4565\n",
      "Yakusoku no Neverland.37779\n",
      "Zoku Natsume Yuujinchou.5300\n",
      "Mushishi: Hihamukage.21329\n",
      "Sora yori mo Tooi Basho.35839\n",
      "Bakuman. 3rd Season.12365\n",
      "Fate/stay night Movie: Heaven's Feel - II. Lost Butterfly.33049\n",
      "Kara no Kyoukai 5: Mujun Rasen.4282\n",
      "Haikyuu!!: To the Top 2nd Season.40776\n",
      "Neon Genesis Evangelion: The End of Evangelion.32\n",
      "Yuru Camp△ Season 2.38474\n",
      "Slam Dunk.170\n",
      "Gintama.: Porori-hen.35843\n",
      "Koukaku Kidoutai: Stand Alone Complex 2nd GIG.801\n",
      "86 Part 2.48569\n",
      "Ansatsu Kyoushitsu 2nd Season.30654\n",
      "Gintama Movie 1: Shinyaku Benizakura-hen.7472\n",
      "Kenpuu Denki Berserk.33\n",
      "One Punch Man.31772\n",
      "Perfect Blue.437\n",
      "Shingeki no Kyojin.16498\n",
      "Re:Zero kara Hajimeru Isekai Seikatsu 2nd Season Part 2.42203\n",
      "Steins;Gate 0.30484\n",
      "Tian Guan Ci Fu Special.44070\n",
      "Uchuu Kyoudai.12431\n",
      "Banana Fish.36649\n",
      "Josee to Tora to Sakana-tachi.40787\n",
      "Aria the Origination.3297\n",
      "Hotaru no Haka.578\n",
      "JoJo no Kimyou na Bouken Part 4: Diamond wa Kudakenai.31933\n",
      "Nana.877\n",
      "Rainbow: Nisha Rokubou no Shichinin.6114\n",
      "Samurai Champloo.205\n",
      "Shingeki no Kyojin Season 2.25777\n",
      "Chihayafuru 3.37379\n",
      "Gintama: The Semi-Final.44087\n",
      "Kono Subarashii Sekai ni Shukufuku wo! Movie: Kurenai Densetsu.38040\n",
      "Mo Dao Zu Shi.37208\n",
      "Mob Psycho 100.37510\n",
      "Vivy: Fluorite Eye's Song.46095\n",
      "Saenai Heroine no Sodatekata Fine.36885\n",
      "Zoku Owarimonogatari.35247\n",
      "Mo Dao Zu Shi: Xian Yun Pian.38450\n",
      "Nichijou.30307\n",
      "Owarimonogatari.35247\n",
      "Steins;Gate Movie: Fuka Ryouiki no Déjà vu.11577\n",
      "Haikyuu!!.20583\n",
      "Mahou Shoujo Madoka★Magica Movie 3: Hangyaku no Monogatari.11981\n",
      "Golden Kamuy 3rd Season.40059\n",
      "Holo no Graffiti.44042\n",
      "Mushishi Zoku Shou: Odoro no Michi.24687\n",
      "Yuu☆Yuu☆Hakusho.392\n",
      "Mononoke.164\n",
      "Saiki Kusuo no Ψ-nan 2.34612\n",
      "Wu Liuqi Zhi Xuanwu Guo Pian.45556\n",
      "Gintama: Yorinuki Gintama-san on Theater 2D.21899\n",
      "JoJo no Kimyou na Bouken Part 3: Stardust Crusaders 2nd Season.26055\n",
      "Kobayashi-san Chi no Maid Dragon S.39247\n",
      "Fumetsu no Anata e.41025\n",
      "Grand Blue.37105\n",
      "Koukaku Kidoutai: Stand Alone Complex.467\n",
      "Saiki Kusuo no Ψ-nan.33255\n",
      "Sayonara no Asa ni Yakusoku no Hana wo Kazarou.35851\n",
      "Natsume Yuujinchou Movie: Utsusemi ni Musubu.36538\n",
      "Kara no Kyoukai 7: Satsujin Kousatsu (Go).5205\n",
      "Kono Oto Tomare! 2nd Season.38889\n",
      "Major S5.36565\n",
      "Gintama°: Aizome Kaori-hen.32366\n",
      "Kaze ga Tsuyoku Fuiteiru.37965\n",
      "Non Non Biyori Nonstop.39808\n",
      "Shingeki no Kyojin: Kuinaki Sentaku.25781\n",
      "Tensei shitara Slime Datta Ken 2nd Season.39551\n",
      "Violet Evergarden Gaiden: Eien to Jidou Shuki Ningyou.39741\n",
      "Natsume Yuujinchou Roku Specials.36275\n",
      "Re:Zero kara Hajimeru Isekai Seikatsu 2nd Season.42203\n",
      "Yahari Ore no Seishun Love Comedy wa Machigatteiru. Kan.39547\n",
      "3-gatsu no Lion.31646\n",
      "Chihayafuru 2.14397\n",
      "Cross Game.5941\n",
      "Houseki no Kuni (TV).35557\n",
      "Hunter x Hunter.11061\n",
      "Kaguya-sama wa Kokurasetai: Tensai-tachi no Renai Zunousen.37999\n",
      "Baccano!.3901\n",
      "Barakamon.22789\n",
      "Kizumonogatari I: Tekketsu-hen.9260\n",
      "Usagi Drop.10162\n",
      "Tian Guan Ci Fu.40730\n",
      "Cowboy Bebop: Tengoku no Tobira.5\n",
      "Kamisama Hajimemashita: Kako-hen.30709\n",
      "Mahou Shoujo Madoka★Magica Movie 2: Eien no Monogatari.11979\n",
      "Bakuman. 2nd Season.10030\n",
      "Hellsing Ultimate.777\n",
      "Kaze no Tani no Nausicaä.572\n",
      "Gotcha!.42984\n",
      "Kingdom 2nd Season.17389\n",
      "Kiseijuu: Sei no Kakuritsu.22535\n",
      "Mahou Shoujo Madoka★Magica.9756\n",
      "Psycho-Pass.23281\n",
      "Yoru Ni Kakeru.48653\n",
      "Mushoku Tensei: Isekai Ittara Honki Dasu.39535\n",
      "Uchuu Senkan Yamato 2199.12029\n",
      "Ano Hi Mita Hana no Namae wo Bokutachi wa Mada Shiranai..9989\n",
      "Bakemonogatari.5081\n",
      "Gintama: Shiroyasha Koutan.6945\n",
      "Given.40421\n",
      "Haikyuu!!: To the Top.38883\n",
      "Major S6.7655\n",
      "K-On! Movie.9617\n",
      "Katanagatari.6594\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No results found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-8f5fcb1a9d47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_anime_df_copy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msearch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAnimeSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmal_id_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmal_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{i}.{search.results[0].mal_id}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mal\\_anime_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, query, timeout)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mAnime\u001b[0m \u001b[0msearch\u001b[0m \u001b[0mby\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"anime\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mal\\_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, query, mal_type, timeout)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_page\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"js-block-list\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No results found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: No results found"
     ]
    }
   ],
   "source": [
    "from mal import AnimeSearch\n",
    "\n",
    "mal_id_list = []\n",
    "counter2 = 0 \n",
    "for i in top_anime_df_copy['Title']:\n",
    "    search = AnimeSearch(i)\n",
    "    try:\n",
    "        mal_id_list.append(search.results[0].mal_id)\n",
    "    except:\n",
    "        print(f\"Faild at {i} !\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mal import Anime\n",
    "\n",
    "genre_list = []\n",
    "scored_by_list = []\n",
    "theme_list = []\n",
    "prducers_list = []\n",
    "\n",
    "\n",
    "for i in mal_id_list:\n",
    "    anime = Anime(i)\n",
    "    genre_list.append(anime.genres)\n",
    "    scored_by_list.append(anime.scored_by)\n",
    "    theme_list.append(anime.themes)\n",
    "    prducers_list.append(anime.prducers)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "list_top_anime_urls = top_anime_df_copy['URL'].to_list()\n",
    "counter3 = 0\n",
    "for anime in list_top_anime_urls:\n",
    "    ### Load the soup object trough load_soup_obj() ###\n",
    "    html_get = requests.get(anime)\n",
    "    t_html_get = html_get.text\n",
    "    if (html_get.status_code != 200):\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception(f\"Failed to fetch web page{anime}\")\n",
    "        \n",
    "    time.sleep(5)\n",
    "    counter3+=1\n",
    "    print(counter3)\n",
    "    soup = BeautifulSoup(t_html_get,'lxml')\n",
    "    soup.title.text.strip()\n",
    "\n",
    "    ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "    headers_mal2 = soup.find_all('td', class_ = 'borderClass')\n",
    "    ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "    row_content2 = soup.find_all('div', {\"class\":'spaceit_pad'})\n",
    "\n",
    "\n",
    "    information = []\n",
    "\n",
    "    for row in row_content2:\n",
    "        if row.find('a') != None:\n",
    "            information.append(row.find('a').text)\n",
    "            \n",
    "    if counter3 == 500 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_anime_df(topic):\n",
    "#     site_url = 'https://myanimelist.net/anime/genre/'\n",
    "    \n",
    "#     def get_topic_page(genre):\n",
    "#         final_url = site_url + genre_dict[genre]+ '/' + genre\n",
    "#         response = requests.get(final_url)\n",
    "#         if response.status_code != 200:\n",
    "#             print('Status code:', response.status_code)\n",
    "            \n",
    "#             raise Exception('Failed to fetch web page ' + final_url)\n",
    "#         return BeautifulSoup(response.text)  \n",
    "    \n",
    "#     doc = get_topic_page(topic)\n",
    "    \n",
    "#     titles = doc.find_all('h2', class_ = 'h2_anime_title')\n",
    "#     producers = doc.find_all('span', class_ ='producer')\n",
    "#     episodes = doc.find_all('div', class_='eps')\n",
    "#     images = doc.find_all('div', class_='image')\n",
    "#     ratings = doc.find_all('div', class_= 'scormem')\n",
    "#     members = doc.find_all('div', class_= 'scormem')\n",
    "    \n",
    "#     def create_lists():\n",
    "#         list_titles = []; list_producer = []; list_episodes = []; list_images = []; list_ratings = []; list_members =[]\n",
    "\n",
    "#         for title in titles:\n",
    "#             name = title.find('a').text\n",
    "#             list_titles.append(name)\n",
    "#         for producer in producers:\n",
    "#             tag = producer.find('a')['title']\n",
    "#             list_producer.append(tag)\n",
    "#         for episode in episodes:\n",
    "#             e = episode.find('span').text\n",
    "#             list_episodes.append(e)\n",
    "#         for img in images:\n",
    "#             i = img.find('img')['data-src']\n",
    "#             list_images.append(i)\n",
    "#         for rating in ratings:\n",
    "#             r = rating.find('span', class_='score').text.strip()\n",
    "#             list_ratings.append(r)\n",
    "#         for member in members:\n",
    "#             m = member.find('span', class_='member').text.strip()\n",
    "#             list_members.append(m)\n",
    "        \n",
    "#         return list_titles, list_producer, list_episodes, list_images , list_ratings , list_members\n",
    "\n",
    "#     if len(titles) == len(producers) == len(episodes) == len(images) == len(ratings) == len(members):\n",
    "#         list_titles, list_producer, list_episodes, list_images , list_ratings , list_members = create_lists()\n",
    "        \n",
    "#         column_names = ['Title', 'Producer', 'Episodes', 'Rating', 'Members','Url']\n",
    "#         df = pd.DataFrame(list(zip(list_titles, list_producer, list_episodes, list_ratings, list_members, list_images)), columns = column_names)\n",
    "        \n",
    "        \n",
    "#     else: print(\"Length of lists are not equal, dataframe cannot be created\")\n",
    "     \n",
    "    \n",
    "#     return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Selenium Go Trough Pages ###\n",
    "\n",
    "# ### Open Certain Page in Selenium ###\n",
    "# PATH = \"C:/Users/Kobi/Downloads/chromedriver.exe\"\n",
    "# driver = webdriver.Chrome(PATH)\n",
    "# driver.get('https://myanimelist.net/topanime.php')\n",
    "\n",
    "# ### Lists ###\n",
    "# top_anime = []\n",
    "# top_anime_Rank = []\n",
    "# top_anime_Title = []\n",
    "# top_anime_Rating = []\n",
    "# top_anime_URL = []\n",
    "# top_anime_Image_URL = []\n",
    "# top_anime_Episodes = []\n",
    "# top_anime_Dates = []\n",
    "\n",
    "\n",
    "# html = driver.page_source\n",
    "# soup = BeautifulSoup(html)\n",
    "\n",
    "# soup.title.text.strip()\n",
    "\n",
    "\n",
    "# ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "# headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "# ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "# row_content = soup.find_all('tr', {'class' : \"ranking-list\"})\n",
    "\n",
    "# ### Fill Lists ###\n",
    "# top_anime_dic = bs_to_dic(row_content=row_content)\n",
    "\n",
    "# time.sleep(2)\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "#     main = WebDriverWait(driver, 20).until(EC.presence_of_element_located(\n",
    "#         (By.CLASS_NAME,'pb12')))\n",
    "#     link = driver.find_element_by_link_text(\"Next 50\")\n",
    "#     link.click()\n",
    "    \n",
    "# finally:\n",
    "#     driver.quit()\n",
    "\n",
    "\n",
    "# # link = driver.find_element_by_link_text('Next 50')\n",
    "# # link.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "# for i,_ in enumerate(top_anime_list):\n",
    "#     html = load_top_anime_html(_)\n",
    "#     res = html_to_lists(html)\n",
    "#     if (i>=1):\n",
    "#         df_new = html_to_lists(html[i])\n",
    "#         df_merged = pd.merge(res,df_new)\n",
    "    \n",
    "\n",
    "#res = load_top_anime(top_anime_list[1])\n",
    "#print(top_anime_list[1])\n",
    "#merged_df = pd.merge(top_anime_df_2,res,on=['Rank', 'Title', 'Rating', 'Image_URL', 'Episodes', 'Dates', 'URL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load the soup object trough load_soup_obj() ###\n",
    "# soup = load_soup_obj('https://myanimelist.net/topanime.php?limit=100')\n",
    "# soup.title.text.strip()\n",
    "# ### find all Rank, Title, Rating, Date of Release, Episodes, Image_Url ###\n",
    "# headers_mal = soup.find('tr', class_ = 'table-header')\n",
    "# ### extract Rank, Title, Rating, Date of Release, Episodes, Image_Url from the given HTML ###\n",
    "# row_content = soup.find_all('tr', {'class' : \"ranking-list\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
